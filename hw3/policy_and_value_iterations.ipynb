{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc44c0b-8b0c-456b-97b9-3d8c0acb84b6",
   "metadata": {},
   "source": [
    "# Практика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bcb413d-d892-40f7-b7c2-71c9f8849a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from Frozen_Lake import FrozenLakeEnv\n",
    "\n",
    "env = FrozenLakeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cea677-bfe6-4029-bd67-fa73931d0d95",
   "metadata": {},
   "source": [
    "Если награда не зависит от $s'$:\n",
    "$$\n",
    "q(s,a) = R(s, a) + \\gamma \\sum_{s'} P(s'|s,a) v(s')\n",
    "$$\n",
    "Если награда зависит от $s'$:\n",
    "$$\n",
    "q(s,a) = \\sum_{s'} P(s'|s,a) \\Big( R(s,a,s') + \\gamma v(s')\\Big)\n",
    "$$\n",
    "Если награда не зависит от $s'$ - это частный случай того, когда награда зависит от $s'$:\n",
    "$$\n",
    "q(s,a) = \\sum_{s'} P(s'|s,a) \\Big( R(s,a) + \\gamma v(s')\\Big) = R(s,a) \\sum_{s'} P(s'|s,a) + \\gamma \\sum_{s'} P(s'|s,a) v(s')\n",
    " = R(s, a) + \\gamma \\sum_{s'} P(s'|s,a) v(s')\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b48c0b58-0990-4957-90cc-3987feeaf1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_values(v_values, gamma):\n",
    "    q_values = {}\n",
    "    for state in env.get_all_states():\n",
    "        q_values[state] = {}\n",
    "        for action in env.get_possible_actions(state):\n",
    "            q_values[state][action] = 0\n",
    "            for next_state in env.get_next_states(state, action):\n",
    "                q_values[state][action] += env.get_transition_prob(state, action, next_state) * env.get_reward(state, action, next_state)\n",
    "                q_values[state][action] += gamma * env.get_transition_prob(state, action, next_state) * v_values[next_state]\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dc79686-d690-4ff3-bbb4-f32e410bb4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_policy():\n",
    "    policy = {}\n",
    "    for state in env.get_all_states():\n",
    "        policy[state] = {}\n",
    "        for action in env.get_possible_actions(state):\n",
    "            policy[state][action] = 1 / len(env.get_possible_actions(state))\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76e7396a-760c-4287-b473-d96064d8d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_v_values():\n",
    "    v_values = {}\n",
    "    for state in env.get_all_states():\n",
    "        v_values[state] = 0\n",
    "    return v_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "348f9a1d-4df4-49fd-92f3-2aceafcf3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_step(v_values, policy, gamma):\n",
    "    q_values = get_q_values(v_values, gamma)\n",
    "    new_v_values = init_v_values()\n",
    "    for state in env.get_all_states():\n",
    "        new_v_values[state] = 0\n",
    "        for action in env.get_possible_actions(state):\n",
    "            new_v_values[state] += policy[state][action] * q_values[state][action]\n",
    "    return new_v_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa145fed-9a3a-49bb-95f8-a6efc7b7f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, gamma, eval_iter_n, v_values=None):\n",
    "    if v_values is None:\n",
    "        v_values = init_v_values()\n",
    "    for _ in range(eval_iter_n):\n",
    "        v_values = policy_evaluation_step(v_values, policy, gamma)\n",
    "    q_values = get_q_values(v_values, gamma)\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7236983d-04c1-4a45-b93d-fb4b10834516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(q_values):\n",
    "    policy = {}\n",
    "    for state in env.get_all_states():\n",
    "        policy[state] = {}\n",
    "        argmax_action = None\n",
    "        max_q_value = float('-inf')\n",
    "        for action in env.get_possible_actions(state): \n",
    "            policy[state][action] = 0\n",
    "            if q_values[state][action] > max_q_value:\n",
    "                argmax_action = action\n",
    "                max_q_value = q_values[state][action]\n",
    "        policy[state][argmax_action] = 1\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e23f38e-0ce9-4265-9c10-2e8a4c320f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_n = 100\n",
    "eval_iter_n = 100\n",
    "gamma = 1.0\n",
    "\n",
    "policy = init_policy()\n",
    "v_values = init_v_values()\n",
    "for _ in range(iter_n):\n",
    "    q_values = policy_evaluation(policy, gamma, eval_iter_n)\n",
    "    policy = policy_improvement(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7259a73b-deed-4675-92c6-329909768f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.988"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    for _ in range(1000):\n",
    "        action = np.random.choice(env.get_possible_actions(state), p=list(policy[state].values()))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c9d126a-3ada-4794-84c0-3b223ee97805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (0, 1): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
       " (0, 2): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (0, 3): {'left': 1, 'down': 0, 'right': 0, 'up': 0},\n",
       " (1, 0): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (1, 1): {None: 1},\n",
       " (1, 2): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (1, 3): {None: 1},\n",
       " (2, 0): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
       " (2, 1): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (2, 2): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
       " (2, 3): {None: 1},\n",
       " (3, 0): {None: 1},\n",
       " (3, 1): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
       " (3, 2): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
       " (3, 3): {None: 1}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7d63fe5-59b5-4e72-863c-1ed3808557e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "*FFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "F*FH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H*FG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF*G\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "for _ in range(1000):\n",
    "    action = np.random.choice(env.get_possible_actions(state), p=list(policy[state].values()))\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3026433-4e18-4f7a-8638-2ed78dd3e56e",
   "metadata": {},
   "source": [
    "# Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf9eed-6747-4f78-b4d8-4191481eff4d",
   "metadata": {},
   "source": [
    "## Задание №3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c213c-c364-4707-a1e7-148a111029c5",
   "metadata": {},
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a21897e-5ae8-409c-8f6e-5911c9712760",
   "metadata": {},
   "source": [
    "1) Вычисление $v_*$ в случае, когда награда зависит от следующего состояния:\n",
    "$$ \n",
    "v_* = max_{a \\in \\mathcal{A}} \\big( R(a) + \\gamma P(a) v_* \\big)\n",
    "$$\n",
    "где\n",
    "$R(a)$ - вектор, состоящий из элементов $R(s_i, a) = \\sum_{s' \\in \\mathcal{S}} P(s' | s_i, a)R(s_i, a, s')$, $1 \\leq i \\leq |\\mathcal{S}|$;\n",
    "$P(a)$ - матрица, состоящая из элементов $P(s_j | s_i, a)$, $1 \\leq i \\leq |\\mathcal{S}|; 1 \\leq j \\leq |\\mathcal{S}|$.\n",
    "\n",
    "2) Вычисление $q_*$:\n",
    "$$\n",
    "q_*(s, a) = \\sum_{s'} P(s' | s, a) \\big( R(s, a, s') + \\gamma v_*(s') \\big)\n",
    "$$\n",
    "или в векторном виде\n",
    "$$\n",
    "q_*(a) = \\sum_{s'} P(s' | a) \\big( R(a, s') + \\gamma v_*(s') \\big)\n",
    "$$\n",
    "3) Итоговый шаг - вычисление policy через теорему greedy policy improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee57310-147a-4568-b880-011d756ad665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from Frozen_Lake import FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d19382-bea5-4c6b-9bf2-5e92f573d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv()\n",
    "POSSIBLE_STATES = tuple(env.get_all_states())\n",
    "POSSIBLE_ACTIONS = (\"left\", \"right\", \"down\", \"up\")\n",
    "N_STATES = len(POSSIBLE_STATES)\n",
    "N_ACTIONS = len(POSSIBLE_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9525ec06-f7d3-4caf-8a37-f135e63ca504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_r_vector():\n",
    "    r = []\n",
    "    for state in POSSIBLE_STATES:\n",
    "        for action in POSSIBLE_ACTIONS:\n",
    "            r_i = 0\n",
    "            # the reward of actions that aren't in possible_actions is considered to be 0\n",
    "            # so we don't need FOR cycle for them\n",
    "            if action in env.get_possible_actions(state):\n",
    "                for next_state, transition_proba in env.get_next_states(\n",
    "                    state, action\n",
    "                ).items():\n",
    "                    r_i += transition_proba * env.get_reward(\n",
    "                        state, action, next_state\n",
    "                    )\n",
    "            r.append(r_i)\n",
    "    # r = [n_states * n_actions]\n",
    "    assert len(r) == N_STATES * N_ACTIONS\n",
    "    return np.array(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c17f415-3051-431a-914c-410f8eb98c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0.8, 0.1, 0.1, 0. , 0. , 0. , 0. ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_r_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3547d3e-f2dc-4d70-9524-51675ddc3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_p_matrix():\n",
    "    p = []\n",
    "    for i_state, state in enumerate(POSSIBLE_STATES):\n",
    "        for action in POSSIBLE_ACTIONS:\n",
    "            if action in env.get_possible_actions(state):\n",
    "                p_row = []\n",
    "                for next_state in POSSIBLE_STATES:\n",
    "                    transition_proba = env.get_next_states(\n",
    "                        state, action\n",
    "                    ).get(next_state, 0.0)\n",
    "                    p_row.append(transition_proba)\n",
    "            else:\n",
    "                p_row = [0.0] * N_STATES\n",
    "                p_row[i_state] = 1.0 \n",
    "            p.append(p_row)\n",
    "    p = np.array(p)\n",
    "    # p = [n_states * n_actions, n_states]\n",
    "    assert p.shape == (N_STATES * N_ACTIONS, N_STATES)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e47ae877-1738-45f4-b01b-59c5ed887fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_p_matrix().reshape((N_STATES, N_ACTIONS, N_STATES)).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af35a7a-2dc0-4cf2-8b48-399ff1ad606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_v_values_step(v_values, r_vector, p_matrix, gamma):\n",
    "    # v_values = [n_states]\n",
    "    # r_vector = [n_states * n_actions]\n",
    "    # p_matrix = [n_states * n_actions, n_states]\n",
    "    raw_v_values = r_vector + gamma * p_matrix.dot(v_values)\n",
    "    # raw_v_values = [n_states * n_actions]\n",
    "    raw_v_values = raw_v_values.reshape((N_STATES, N_ACTIONS))\n",
    "    v_values = raw_v_values.max(axis=-1)\n",
    "    # v_values = [n_states]\n",
    "    return v_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6750b35b-1ae0-4937-8921-7825c4f30080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_v_values(n_iterations, gamma):\n",
    "    v_values = np.zeros((N_STATES,))\n",
    "    r_vector, p_matrix = calc_r_vector(), calc_p_matrix()\n",
    "    # r_vector = [n_states * n_action]\n",
    "    # p_matrix = [n_states * n_actions, n_states]\n",
    "    for i_iteration in range(n_iterations):\n",
    "        v_values = calc_v_values_step(v_values, r_vector, p_matrix, gamma)\n",
    "    # v_values = [n_states]\n",
    "    return v_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abfe4908-a7ba-4c1b-b8e2-a2a55d2b681a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9962074 , 0.99604792, 0.9959365 , 0.99587922, 0.99623226,\n",
       "       0.        , 0.79674484, 0.        , 0.9962951 , 0.99683244,\n",
       "       0.97710783, 0.        , 0.        , 0.99936583, 0.99968264,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_values = calc_v_values(n_iterations=1000, gamma=1.0)\n",
    "v_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8652ac24-7964-4b3c-a2ad-35ef7ec415f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rewards_matrix():\n",
    "    rewards = []\n",
    "    for state in POSSIBLE_STATES:\n",
    "        for action in POSSIBLE_ACTIONS:\n",
    "            rewards_row = []\n",
    "            for next_state in POSSIBLE_STATES:\n",
    "                if (\n",
    "                    action in env.get_possible_actions(state)\n",
    "                    and next_state in env.get_next_states(state, action)\n",
    "                ):\n",
    "                    reward = env.get_reward(state, action, next_state)\n",
    "                else:\n",
    "                    reward = 0\n",
    "                rewards_row.append(reward)\n",
    "            rewards.append(rewards_row)\n",
    "    rewards = np.array(rewards)\n",
    "    # rewards = [n_states * n_actions, n_states]\n",
    "    assert rewards.shape == (N_STATES * N_ACTIONS, N_STATES)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d4b6bd5-bdb9-4b79-9a97-a78a0246105d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_rewards_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "befbf212-3e39-418a-aa61-45b588e5b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_q_values(v_values, gamma):\n",
    "    p_matrix = calc_p_matrix()\n",
    "    # p_matrix = [n_states * n_actions, n_states]\n",
    "    rewards = calc_rewards_matrix()\n",
    "    # rewards = [n_states * n_actions, n_states]\n",
    "    raw_q_values = gamma * p_matrix.dot(v_values) + (p_matrix * rewards).sum(-1)\n",
    "    # raw_q_values = [n_states * n_actions]\n",
    "    q_values = raw_q_values.reshape((N_STATES, N_ACTIONS,))\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "707180c9-10fb-4ed1-b7bd-d01b8f02f534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99620989, 0.9960823 , 0.99621134, 0.99619145],\n",
       "       [0.89657071, 0.89635399, 0.19921439, 0.99605272],\n",
       "       [0.97610647, 0.97597151, 0.83658859, 0.99594191],\n",
       "       [0.89633712, 0.8962913 , 0.19918157, 0.99588495],\n",
       "       [0.99623606, 0.19925025, 0.89665931, 0.89658915],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.19730443, 0.19730443, 0.78168627, 0.7967492 ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.89665931, 0.89708917, 0.19931275, 0.99629856],\n",
       "       [0.89697267, 0.88162285, 0.99683296, 0.19734029],\n",
       "       [0.9771087 , 0.17964275, 0.89942935, 0.73707912],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.19961983, 0.99936594, 0.89946093, 0.89743421],\n",
       "       [0.99717171, 0.99767905, 0.99968269, 0.98162285],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values = calc_q_values(v_values, gamma=1.0)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93bd0ab0-e102-4fbe-932c-b2edc94b7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_policy(q_values):\n",
    "    policy = dict()\n",
    "    for state, state_q_values in zip(POSSIBLE_STATES, q_values):\n",
    "        policy[state] = dict()\n",
    "        argmax_action = POSSIBLE_ACTIONS[state_q_values.argmax()]\n",
    "        for action in env.get_possible_actions(state):\n",
    "            if action == argmax_action:\n",
    "                policy[state][action] = 1.0\n",
    "            else:\n",
    "                policy[state][action] = 0.0\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe0c9257-dc9f-4302-9d9f-5273398db1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (0, 1): {'left': 0.0, 'down': 0.0, 'right': 0.0, 'up': 1.0},\n",
       " (0, 2): {'left': 0.0, 'down': 0.0, 'right': 0.0, 'up': 1.0},\n",
       " (0, 3): {'left': 0.0, 'down': 0.0, 'right': 0.0, 'up': 1.0},\n",
       " (1, 0): {'left': 1.0, 'down': 0.0, 'right': 0.0, 'up': 0.0},\n",
       " (1, 1): {},\n",
       " (1, 2): {'left': 0.0, 'down': 0.0, 'right': 0.0, 'up': 1.0},\n",
       " (1, 3): {},\n",
       " (2, 0): {'left': 0.0, 'down': 0.0, 'right': 0.0, 'up': 1.0},\n",
       " (2, 1): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (2, 2): {'left': 1.0, 'down': 0.0, 'right': 0.0, 'up': 0.0},\n",
       " (2, 3): {},\n",
       " (3, 0): {},\n",
       " (3, 1): {'left': 0.0, 'down': 0.0, 'right': 1.0, 'up': 0.0},\n",
       " (3, 2): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (3, 3): {}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_policy(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8f0f350-a13d-440d-974c-8de57f25e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(n_iterations, gamma):\n",
    "    v_values = calc_v_values(n_iterations, gamma)\n",
    "    q_values = calc_q_values(v_values, gamma)\n",
    "    return calc_policy(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ad3042d-66c5-4b48-9cff-bdbe4d866b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (0, 1): {'left': 0.0, 'down': 0.0, 'right': 0.0, 'up': 1.0},\n",
       " (0, 2): {'left': 0.0, 'down': 0.0, 'right': 0.0, 'up': 1.0},\n",
       " (0, 3): {'left': 0.0, 'down': 0.0, 'right': 0.0, 'up': 1.0},\n",
       " (1, 0): {'left': 1.0, 'down': 0.0, 'right': 0.0, 'up': 0.0},\n",
       " (1, 1): {},\n",
       " (1, 2): {'left': 0.0, 'down': 0.0, 'right': 0.0, 'up': 1.0},\n",
       " (1, 3): {},\n",
       " (2, 0): {'left': 0.0, 'down': 0.0, 'right': 0.0, 'up': 1.0},\n",
       " (2, 1): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (2, 2): {'left': 1.0, 'down': 0.0, 'right': 0.0, 'up': 0.0},\n",
       " (2, 3): {},\n",
       " (3, 0): {},\n",
       " (3, 1): {'left': 0.0, 'down': 0.0, 'right': 1.0, 'up': 0.0},\n",
       " (3, 2): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (3, 3): {}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = value_iteration(n_iterations=1000, gamma=1.0)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e1a31-cab8-44a5-8e5c-cbac62b90f2f",
   "metadata": {},
   "source": [
    "### Оценка качества политики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85d18b7d-f016-488b-bc21-07dac53dbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy):\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        for _ in range(1000):\n",
    "            action = np.random.choice(\n",
    "                POSSIBLE_ACTIONS, \n",
    "                p=[policy[state][action] for action in POSSIBLE_ACTIONS]\n",
    "            )\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    return np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac4f6cd8-dc54-4388-97ee-c56fc89eea39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26fee9be-9ed3-40c8-9c0c-52e3f967015f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_agent(policy):\n",
    "    state = env.reset()\n",
    "    for _ in range(1000):\n",
    "        action = np.random.choice(\n",
    "            POSSIBLE_ACTIONS, \n",
    "            p=[policy[state][action] for action in POSSIBLE_ACTIONS]\n",
    "        )\n",
    "        state, _, done, _ = env.step(action)\n",
    "    \n",
    "        env.render()\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e44d446b-0602-4549-9b21-210b4fa09ffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "*FFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "*FFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "*FFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "*FFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "*FFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "*FFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "*FFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "*FFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "*FFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "*FFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "F*FH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FF*H\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "F*FH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H*FG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF*G\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF*G\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF*G\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF*G\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF*G\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF*G\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF*G\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_agent(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17122a71-b319-4bcb-a6f1-a86b0a57310b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 0): 0.1, (2, 0): 0.8, (1, 1): 0.1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_next_states((1, 0), \"down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6032a8f-5e4f-496a-b445-ec6dd2526e90",
   "metadata": {},
   "source": [
    "### Эксперименты со средой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "447dd4db-8144-4fb4-8dc6-de54cc44563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFrozenLakeEnv(FrozenLakeEnv):\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        reward = super().get_reward(state, action, next_state)\n",
    "        if reward == 0:\n",
    "            reward = -1\n",
    "        elif reward == 1:\n",
    "            reward = 10\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3f3a0b8-d62b-41c6-8766-1e9b9be17ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomFrozenLakeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae010196-6c51-44d0-8bc3-d6bbda9a794e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (0, 1): {'left': 0.0, 'down': 0.0, 'right': 1.0, 'up': 0.0},\n",
       " (0, 2): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (0, 3): {'left': 1.0, 'down': 0.0, 'right': 0.0, 'up': 0.0},\n",
       " (1, 0): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (1, 1): {},\n",
       " (1, 2): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (1, 3): {},\n",
       " (2, 0): {'left': 0.0, 'down': 0.0, 'right': 1.0, 'up': 0.0},\n",
       " (2, 1): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (2, 2): {'left': 0.0, 'down': 1.0, 'right': 0.0, 'up': 0.0},\n",
       " (2, 3): {},\n",
       " (3, 0): {},\n",
       " (3, 1): {'left': 0.0, 'down': 0.0, 'right': 1.0, 'up': 0.0},\n",
       " (3, 2): {'left': 0.0, 'down': 0.0, 'right': 1.0, 'up': 0.0},\n",
       " (3, 3): {}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = value_iteration(n_iterations=1000, gamma=1.0)\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "362393ea-6a2b-4338-be82-be7576b3522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1): 0.1, (2, 2): 0.8, (1, 3): 0.1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_next_states((1, 2), \"down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2076b1c-6381-41fc-831f-7bd0ee339741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.618"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "528e10a0-9604-45c5-b90e-d5a1e39773ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "*FFH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "F*FH\n",
      "HFFG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H*FG\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF*G\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_agent(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc68c5c-bb5e-4b4e-aa40-1e7c998936b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
